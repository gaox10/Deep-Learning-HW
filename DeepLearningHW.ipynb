{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['GrLivArea','GrLivArea','TotalBsmtSF','GarageArea', 'GarageCars','1stFlrSF','YearBuilt' ]\n",
    "X=train[cols].values\n",
    "y=train['SalePrice'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1022, 7) (438, 7) (1022,) (438,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "size_hidden= 100\n",
    "\n",
    "#Calculate some other hyperparameters based on data.  \n",
    "batch_no = len(X_train) // batch_size  #batches\n",
    "cols=X_train.shape[1] #Number of columns in input matrix\n",
    "n_output=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the model on : cpu\n"
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(\"Executing the model on :\",device)\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, size_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "net = Net(cols, size_hidden, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "criterion = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  1592797802496.0\n",
      "Epoch 2 loss:  1604695711744.0\n",
      "Epoch 3 loss:  1607026186240.0\n",
      "Epoch 4 loss:  1565768525824.0\n",
      "Epoch 5 loss:  1571922726912.0\n",
      "Epoch 6 loss:  1551612282880.0\n",
      "Epoch 7 loss:  1564204185600.0\n",
      "Epoch 8 loss:  1584044828672.0\n",
      "Epoch 9 loss:  1582290145280.0\n",
      "Epoch 10 loss:  1504485482496.0\n",
      "Epoch 11 loss:  1560348444672.0\n",
      "Epoch 12 loss:  1571179534336.0\n",
      "Epoch 13 loss:  1597839761408.0\n",
      "Epoch 14 loss:  1616968495104.0\n",
      "Epoch 15 loss:  1565251264512.0\n",
      "Epoch 16 loss:  1602373332992.0\n",
      "Epoch 17 loss:  1565539299328.0\n",
      "Epoch 18 loss:  1578754326528.0\n",
      "Epoch 19 loss:  1573426155520.0\n",
      "Epoch 20 loss:  1573728473088.0\n",
      "Epoch 21 loss:  1540587433984.0\n",
      "Epoch 22 loss:  1627643867136.0\n",
      "Epoch 23 loss:  1636139950080.0\n",
      "Epoch 24 loss:  1565100642304.0\n",
      "Epoch 25 loss:  1553282721792.0\n",
      "Epoch 26 loss:  1552217600000.0\n",
      "Epoch 27 loss:  1590128955392.0\n",
      "Epoch 28 loss:  1597101690880.0\n",
      "Epoch 29 loss:  1608952606720.0\n",
      "Epoch 30 loss:  1640565219328.0\n",
      "Epoch 31 loss:  1641555484672.0\n",
      "Epoch 32 loss:  1597311885312.0\n",
      "Epoch 33 loss:  1612311764992.0\n",
      "Epoch 34 loss:  1567833550848.0\n",
      "Epoch 35 loss:  1586653497344.0\n",
      "Epoch 36 loss:  1600660754432.0\n",
      "Epoch 37 loss:  1554330861568.0\n",
      "Epoch 38 loss:  1503214391296.0\n",
      "Epoch 39 loss:  1599772131328.0\n",
      "Epoch 40 loss:  1558310443008.0\n",
      "Epoch 41 loss:  1588408051712.0\n",
      "Epoch 42 loss:  1561916289024.0\n",
      "Epoch 43 loss:  1548113752064.0\n",
      "Epoch 44 loss:  1587394017280.0\n",
      "Epoch 45 loss:  1519107420160.0\n",
      "Epoch 46 loss:  1528695420928.0\n",
      "Epoch 47 loss:  1558857551872.0\n",
      "Epoch 48 loss:  1559176769536.0\n",
      "Epoch 49 loss:  1555359666176.0\n",
      "Epoch 50 loss:  1584846557184.0\n",
      "Epoch 51 loss:  1578063347712.0\n",
      "Epoch 52 loss:  1566754168832.0\n",
      "Epoch 53 loss:  1574749460480.0\n",
      "Epoch 54 loss:  1513436149760.0\n",
      "Epoch 55 loss:  1592602611712.0\n",
      "Epoch 56 loss:  1591295918080.0\n",
      "Epoch 57 loss:  1578006882304.0\n",
      "Epoch 58 loss:  1555129167872.0\n",
      "Epoch 59 loss:  1557356695552.0\n",
      "Epoch 60 loss:  1578067689472.0\n",
      "Epoch 61 loss:  1594799448064.0\n",
      "Epoch 62 loss:  1586759446528.0\n",
      "Epoch 63 loss:  1558814720000.0\n",
      "Epoch 64 loss:  1541403410432.0\n",
      "Epoch 65 loss:  1623555430400.0\n",
      "Epoch 66 loss:  1550210940928.0\n",
      "Epoch 67 loss:  1580368994304.0\n",
      "Epoch 68 loss:  1412859437056.0\n",
      "Epoch 69 loss:  1431566243840.0\n",
      "Epoch 70 loss:  1588790132736.0\n",
      "Epoch 71 loss:  1571133472768.0\n",
      "Epoch 72 loss:  1559895412736.0\n",
      "Epoch 73 loss:  1545410766848.0\n",
      "Epoch 74 loss:  1579120015360.0\n",
      "Epoch 75 loss:  1586584117248.0\n",
      "Epoch 76 loss:  1556438024192.0\n",
      "Epoch 77 loss:  1561480974336.0\n",
      "Epoch 78 loss:  1581814194176.0\n",
      "Epoch 79 loss:  1524149129216.0\n",
      "Epoch 80 loss:  1600581509120.0\n",
      "Epoch 81 loss:  1587559792640.0\n",
      "Epoch 82 loss:  1645361262592.0\n",
      "Epoch 83 loss:  1578639368192.0\n",
      "Epoch 84 loss:  1579838087168.0\n",
      "Epoch 85 loss:  1566985533440.0\n",
      "Epoch 86 loss:  1590430453760.0\n",
      "Epoch 87 loss:  1572513439744.0\n",
      "Epoch 88 loss:  1603517059072.0\n",
      "Epoch 89 loss:  1572128051200.0\n",
      "Epoch 90 loss:  1577051189248.0\n",
      "Epoch 91 loss:  1603661008896.0\n",
      "Epoch 92 loss:  1539484112896.0\n",
      "Epoch 93 loss:  1557114773504.0\n",
      "Epoch 94 loss:  1575034497024.0\n",
      "Epoch 95 loss:  1565570875392.0\n",
      "Epoch 96 loss:  1563071668224.0\n",
      "Epoch 97 loss:  1591841951744.0\n",
      "Epoch 98 loss:  1572046872576.0\n",
      "Epoch 99 loss:  1541525524480.0\n",
      "Epoch 100 loss:  1566839746560.0\n",
      "Epoch 101 loss:  1584846077952.0\n",
      "Epoch 102 loss:  1579113607168.0\n",
      "Epoch 103 loss:  1565331697664.0\n",
      "Epoch 104 loss:  1400545128448.0\n",
      "Epoch 105 loss:  1584408842240.0\n",
      "Epoch 106 loss:  1604884824064.0\n",
      "Epoch 107 loss:  1585157210112.0\n",
      "Epoch 108 loss:  1542560022528.0\n",
      "Epoch 109 loss:  1532920696832.0\n",
      "Epoch 110 loss:  1581863047168.0\n",
      "Epoch 111 loss:  1582801358848.0\n",
      "Epoch 112 loss:  1579965505536.0\n",
      "Epoch 113 loss:  1593409589248.0\n",
      "Epoch 114 loss:  1600927653888.0\n",
      "Epoch 115 loss:  1449063096320.0\n",
      "Epoch 116 loss:  1504602095616.0\n",
      "Epoch 117 loss:  1550618322944.0\n",
      "Epoch 118 loss:  1572670543872.0\n",
      "Epoch 119 loss:  1549252284416.0\n",
      "Epoch 120 loss:  1562146529280.0\n",
      "Epoch 121 loss:  1552852086784.0\n",
      "Epoch 122 loss:  1583279878144.0\n",
      "Epoch 123 loss:  1576940679168.0\n",
      "Epoch 124 loss:  1407739432960.0\n",
      "Epoch 125 loss:  1600189440000.0\n",
      "Epoch 126 loss:  1584598337536.0\n",
      "Epoch 127 loss:  1578315706368.0\n",
      "Epoch 128 loss:  1576039497728.0\n",
      "Epoch 129 loss:  1636401197056.0\n",
      "Epoch 130 loss:  1585649299456.0\n",
      "Epoch 131 loss:  1545601617920.0\n",
      "Epoch 132 loss:  1568632445952.0\n",
      "Epoch 133 loss:  1580026863616.0\n",
      "Epoch 134 loss:  1552906801152.0\n",
      "Epoch 135 loss:  1510558509056.0\n",
      "Epoch 136 loss:  1601215602688.0\n",
      "Epoch 137 loss:  1649073758208.0\n",
      "Epoch 138 loss:  1531412926464.0\n",
      "Epoch 139 loss:  1602870038528.0\n",
      "Epoch 140 loss:  1554787600384.0\n",
      "Epoch 141 loss:  1603518013440.0\n",
      "Epoch 142 loss:  1552747995136.0\n",
      "Epoch 143 loss:  1563123165184.0\n",
      "Epoch 144 loss:  1578127876096.0\n",
      "Epoch 145 loss:  1519290073088.0\n",
      "Epoch 146 loss:  1571329531904.0\n",
      "Epoch 147 loss:  1566495995904.0\n",
      "Epoch 148 loss:  1561191725056.0\n",
      "Epoch 149 loss:  1615930630144.0\n",
      "Epoch 150 loss:  1554593259520.0\n",
      "Epoch 151 loss:  1562773403648.0\n",
      "Epoch 152 loss:  1585880989696.0\n",
      "Epoch 153 loss:  1568599580672.0\n",
      "Epoch 154 loss:  1554266218496.0\n",
      "Epoch 155 loss:  1549302681600.0\n",
      "Epoch 156 loss:  1602590593024.0\n",
      "Epoch 157 loss:  1605958807552.0\n",
      "Epoch 158 loss:  1588843614208.0\n",
      "Epoch 159 loss:  1558573604864.0\n",
      "Epoch 160 loss:  1577030965248.0\n",
      "Epoch 161 loss:  1390225506304.0\n",
      "Epoch 162 loss:  1530881331200.0\n",
      "Epoch 163 loss:  1564234123264.0\n",
      "Epoch 164 loss:  1567749240832.0\n",
      "Epoch 165 loss:  1544851132416.0\n",
      "Epoch 166 loss:  1561825374208.0\n",
      "Epoch 167 loss:  1545864437760.0\n",
      "Epoch 168 loss:  1579658301440.0\n",
      "Epoch 169 loss:  1589533321216.0\n",
      "Epoch 170 loss:  1574667501568.0\n",
      "Epoch 171 loss:  1567824953344.0\n",
      "Epoch 172 loss:  1543411087360.0\n",
      "Epoch 173 loss:  1548537214976.0\n",
      "Epoch 174 loss:  1530813890560.0\n",
      "Epoch 175 loss:  1628865542144.0\n",
      "Epoch 176 loss:  1603245281280.0\n",
      "Epoch 177 loss:  1564906602496.0\n",
      "Epoch 178 loss:  1554247196672.0\n",
      "Epoch 179 loss:  1601201065984.0\n",
      "Epoch 180 loss:  1562273538048.0\n",
      "Epoch 181 loss:  1520339767296.0\n",
      "Epoch 182 loss:  1576330596352.0\n",
      "Epoch 183 loss:  1507478421504.0\n",
      "Epoch 184 loss:  1550976131072.0\n",
      "Epoch 185 loss:  1564663543808.0\n",
      "Epoch 186 loss:  1559654993920.0\n",
      "Epoch 187 loss:  1578732756992.0\n",
      "Epoch 188 loss:  1606066737152.0\n",
      "Epoch 189 loss:  1581556588544.0\n",
      "Epoch 190 loss:  1570661146624.0\n",
      "Epoch 191 loss:  1568076075008.0\n",
      "Epoch 192 loss:  1530102941696.0\n",
      "Epoch 193 loss:  1511651756032.0\n",
      "Epoch 194 loss:  1577107488768.0\n",
      "Epoch 195 loss:  1575826132992.0\n",
      "Epoch 196 loss:  1636339183616.0\n",
      "Epoch 197 loss:  1554184892416.0\n",
      "Epoch 198 loss:  1576280672256.0\n",
      "Epoch 199 loss:  1527482537984.0\n",
      "Epoch 200 loss:  1603281567744.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    #Shuffle just mixes up the dataset between epocs\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    # Mini batch learning\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.FloatTensor(X_train[start:end]))\n",
    "        labels = Variable(torch.FloatTensor(y_train[start:end]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels,dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022 1022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6645816518425565"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = Variable(torch.FloatTensor(X_train)) \n",
    "result = net(X)\n",
    "pred=result.data[:,0].numpy()\n",
    "print(len(pred),len(y_train))\n",
    "r2_score(pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "#This is a little bit tricky to get the resulting prediction.  \n",
    "def calculate_r2(x,y=[]):\n",
    "    \"\"\"\n",
    "    This function will return the r2 if passed x and y or return predictions if just passed x. \n",
    "    \"\"\"\n",
    "    # Evaluate the model with the test set. \n",
    "    X = Variable(torch.FloatTensor(x))  \n",
    "    result = net(X) #This outputs the value for regression\n",
    "    result=result.data[:,0].numpy()\n",
    "  \n",
    "    if len(y) != 0:\n",
    "        r2=r2_score(result, y)\n",
    "        print(\"R-Squared\", r2)\n",
    "        #print('Accuracy {:.2f}'.format(num_right / len(y)), \"for a total of \", len(y), \"records\")\n",
    "        return pd.DataFrame(data= {'actual': y, 'predicted': result})\n",
    "    else:\n",
    "        print(\"returning predictions\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared 0.6645816518425565\n",
      "R-Squared 0.4862489869095332\n"
     ]
    }
   ],
   "source": [
    "result1=calculate_r2(X_train,y_train)\n",
    "result2=calculate_r2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
